{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from scipy.cluster.vq import whiten, kmeans, kmeans2\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "rng = default_rng(900)\n",
    "\n",
    "cuda = True\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network parameters\n",
    "mb_size = 256 # mini batch dim\n",
    "h_Q_dim = 512*4 # encoder dim\n",
    "h_P_dim = 512*4 # decoder dim\n",
    "\n",
    "lr = 1e-4 # learning rate\n",
    "\n",
    "# problem dimenc_dimsions\n",
    "nDrawnSamples = 21 # number of dependent samples to draw during smapling (length of predicted control series)\n",
    "nPreviousStates = 1 # number of previous states to condition on the steering prediction\n",
    "dim = 4 # (x, y, yaw, steer)\n",
    "#dataElements = nDrawnSamples + 2 * (dim - 1) # steer sample, current pose (3D: x,y,yaw), goal (3D)\n",
    "dataElements = (nDrawnSamples + nPreviousStates) * dim + dim - 1\n",
    "\n",
    "z_dim = 1 # latent dim\n",
    "X_dim = nDrawnSamples # steering\n",
    "y_dim = dim # renp.piruction of the original point (unsused?)\n",
    "c_dim = dataElements - X_dim*dim # dimension of conditioning variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load map file\n",
    "\n",
    "filename = '/home/oscar_palfelt/MSc_thesis/EECS_Degree_Project/learn_control/motion_planning/maps/map_difficult.png'\n",
    "mapImg = cv2.imread(filename, 0)\n",
    "blurImg = cv2.blur(mapImg, ksize=(3,3))\n",
    "occGrid = np.clip(mapImg, 0, 1)\n",
    "inflatedGrid = np.floor(blurImg / 255)\n",
    "\n",
    "assert occGrid.shape[0] == occGrid.shape[1]\n",
    "gridSize = occGrid.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97029, 91)\n"
     ]
    }
   ],
   "source": [
    "# read in data from .txt file, re-arrange to allow drawing multiple depedent samples\n",
    "\n",
    "filename = '/home/oscar_palfelt/MSc_thesis/EECS_Degree_Project/learn_control/motion_planning/data/ctrlDataPfollow'\n",
    "rawdata = np.genfromtxt(filename, delimiter=',', dtype='d', usecols=[0,1,2,3,7,8,9]) # disregard init\n",
    "\n",
    "_, pathsIdx = np.unique(rawdata[:,4:], axis=0, return_index=True)\n",
    "pathsIdx.sort()\n",
    "\n",
    "pathsLengths = np.roll(pathsIdx, -1) - pathsIdx\n",
    "pathsLengths[-1] = rawdata.shape[0] - np.sum(pathsLengths[:-1])\n",
    "\n",
    "validLengthsIdx = np.argwhere(pathsLengths >= nPreviousStates + nDrawnSamples) \n",
    "validPlansIdx = pathsIdx[validLengthsIdx]\n",
    "\n",
    "data = np.zeros(shape=(1, dataElements)) # steering angles, samples (x,y,yaw), goal(x,y,yaw)\n",
    "tempdata = np.copy(data) # steering angles, samples (x,y,yaw), goal(x,y,yaw)\n",
    "for ci, i in enumerate(validPlansIdx.reshape(-1)):\n",
    "    #rawdata[i:i+pathsLenghts[ci], dim-1] = uniform_filter1d(rawdata[i:i+pathsLenghts[ci], dim-1], size=1, origin=0, mode='nearest', axis=0) # average over steering\n",
    "    for j in range(pathsLengths[ci] - (nPreviousStates + nDrawnSamples - 1)):\n",
    "        sample = np.arange(start=j, stop=j + nPreviousStates + nDrawnSamples)\n",
    "\n",
    "        if np.any(np.abs(np.diff(rawdata[i + sample, 0])) > 0.05) or np.any(np.abs(np.diff(rawdata[i + sample, 1])) > 0.05):\n",
    "            break # ignore duplicate plans (data generation is not perfect)\n",
    "\n",
    "        tempdata = np.vstack((tempdata, np.append(rawdata[i + sample, :dim].reshape(1, (nPreviousStates + nDrawnSamples)*dim), rawdata[i, dim:].reshape(1, dim-1), axis=1))) \n",
    "        if ci % 500 == 0:\n",
    "            data = np.vstack((data, tempdata[1:]))\n",
    "            tempdata = np.zeros(shape=(1, dataElements))\n",
    "\n",
    "rng.shuffle(data) # shuffle data\n",
    "dataNonscaled = np.copy(data)\n",
    "prevStatesConditionsNonscaled = np.copy(data[:, :nPreviousStates * dim])\n",
    "data[:, dim-2::dim] /= np.max(np.abs(data[:, dim-2::dim]),axis=0) # normalize yaw angles to [-1, 1]\n",
    "data[:, dim-1::dim] /= np.max(np.abs(data[:, dim-1::dim]),axis=0) # normalzie steering angles to [-1, 1]\n",
    "data = data[1:]\n",
    "\n",
    "numEntries = data.shape[0]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 719 # np.random.randint(1,numEntries)\n",
    "plt.imshow(mapImg, extent=[0, gridSize, 0, gridSize], cmap='gray')\n",
    "plt.scatter(data[i, :-dim:dim] * gridSize, data[i, 1:-dim:dim] * gridSize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77623, 21)\n"
     ]
    }
   ],
   "source": [
    "# split the inputs and conditions into test train (to be processed in the next step into an occupancy grid representation)\n",
    "\n",
    "ratioTestTrain = 0.8;\n",
    "numTrain = int(numEntries*ratioTestTrain)\n",
    "\n",
    "X_train = data[0:numTrain, dim * nPreviousStates + dim-1::dim] # steering angles to predict\n",
    "X_test = data[numTrain:numEntries, dim * nPreviousStates + dim-1::dim]\n",
    "\n",
    "numTest = X_test.shape[0]\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97029, 7)\n"
     ]
    }
   ],
   "source": [
    "# generate conditioning variable\n",
    "\n",
    "prevStatesNonscaled = dataNonscaled[1:, :nPreviousStates*dim]\n",
    "nextStatesNonscaled = dataNonscaled[1:, nPreviousStates*dim:-dim+1]\n",
    "nextStates = data[:, nPreviousStates*dim:-dim+1]\n",
    "goalStateNonscaled = dataNonscaled[1:, -dim+1:]\n",
    "\n",
    "prevStatesConditions = data[:, :nPreviousStates * dim]\n",
    "goalStateCondition = data[:, -dim+1:]\n",
    "\n",
    "cs = np.concatenate((prevStatesConditions, goalStateCondition), axis=1)\n",
    "\n",
    "c_dim = cs.shape[1]\n",
    "c_train = cs[0:numTrain,:]  \n",
    "c_test = cs[numTrain:numEntries,:]\n",
    "\n",
    "print(cs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pytorch networks\n",
    "# based on https://github.com/Jackson-Kang/Pytorch-VAE-tutorial/blob/master/.ipynb_checkpoints/01_Variational_AutoEncoder-checkpoint.ipynb\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=X_dim+c_dim, hidden_dim=h_Q_dim, latent_dim=z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.z_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        seq = self.network(x)\n",
    "\n",
    "        return self.z_mu(seq), self.z_logvar(seq)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=z_dim+c_dim, hidden_dim=h_P_dim, output_dim=X_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "    \n",
    "    def reparameterization(self, mean, logvar):\n",
    "        epsilon = torch.randn_like(mean).to(DEVICE)        # sampling epsilon        \n",
    "        z = mean + torch.exp(0.5 * logvar) * epsilon       # reparameterization trick\n",
    "        return z\n",
    "\n",
    "    def forward(self, x, c, encode=True):\n",
    "        if encode:\n",
    "            z_mu, z_logvar = self.Encoder(torch.cat((x, c), dim=1))\n",
    "            z = self.reparameterization(z_mu, z_logvar)\n",
    "\n",
    "            y = self.Decoder(torch.cat((z, c), dim=1))\n",
    "            \n",
    "            return y, z_mu, z_logvar\n",
    "        else:\n",
    "            z = x\n",
    "            y = self.Decoder(torch.cat((z, c), dim=1))    \n",
    "\n",
    "            return y\n",
    "\n",
    "\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "network = NeuralNetwork(Encoder=encoder, Decoder=decoder).to(DEVICE)\n",
    "\n",
    "def loss_function(x, y, mean, logvar):\n",
    "    recon_loss = ((x - y) ** 2).mean()\n",
    "    kl_loss    = 10**-4 * 2 * torch.sum(torch.exp(logvar) + mean.pow(2) - 1. - logvar, dim=1)\n",
    "\n",
    "    return torch.mean(kl_loss + recon_loss)\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for it in range(120001):\n",
    "    # randomly generate batches\n",
    "    batch_elements = [np.random.randint(0,numTrain-1) for n in range(0,mb_size)]\n",
    "\n",
    "    X_mb = torch.tensor(X_train[batch_elements,:], requires_grad=True, dtype=torch.float32, device=DEVICE)\n",
    "    c_mb = torch.tensor(c_train[batch_elements,:], requires_grad=True, dtype=torch.float32, device=DEVICE)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y, z_mu, z_logvar = network(X_mb, c_mb)\n",
    "    \n",
    "    loss = loss_function(X_mb, y, z_mu, z_logvar)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        batch_elements = [np.random.randint(0,numTest-1) for n in range(0,mb_size)]\n",
    "\n",
    "        Xval_mb = torch.tensor(X_test[batch_elements,:], requires_grad=False, dtype=torch.float32, device=DEVICE)\n",
    "        cval_mb = torch.tensor(c_test[batch_elements,:], requires_grad=False, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        yval, z_muval, z_logvarval = network(Xval_mb, cval_mb)\n",
    "\n",
    "        valloss = loss_function(Xval_mb, yval, z_muval, z_logvarval)\n",
    "\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Loss: {:.4}'. format(loss))\n",
    "        print('VLoss: {:.4}'. format(valloss))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the latent space\n",
    "num_viz = 800 # number of samples to draw in latent space\n",
    "\n",
    "vizIdx += 50 # np.random.randint(0,numTest-1)\n",
    "c_sample_seed = c_test[vizIdx,:]\n",
    "c_sample = torch.from_numpy(np.repeat([c_sample_seed],num_viz,axis=0)).float().to(DEVICE)\n",
    "\n",
    "y_viz = network(torch.randn(num_viz, z_dim).to(DEVICE), c_sample, encode=False).cpu().detach().numpy() # y_viz is sample in state space, zviz in latent space (3D)\n",
    "t = np.tile(np.arange(nDrawnSamples), reps=(num_viz, 1)) * 0.05\n",
    "\n",
    "xPrev = prevStatesNonscaled[numTrain + vizIdx, ::dim]\n",
    "yPrev = prevStatesNonscaled[numTrain + vizIdx, 1::dim]\n",
    "yawPrev = prevStatesNonscaled[numTrain + vizIdx, 2::dim]\n",
    "steerPrev = prevStatesConditions[numTrain + vizIdx, 3::dim]\n",
    "\n",
    "xNext = nextStatesNonscaled[numTrain + vizIdx, ::dim]\n",
    "yNext = nextStatesNonscaled[numTrain + vizIdx, 1::dim]\n",
    "steerNext = nextStates[numTrain + vizIdx, 3::dim]\n",
    "\n",
    "plt.quiver(xPrev[-1] * gridSize, yPrev[[-1]] * gridSize, np.cos(yawPrev[-1]), np.sin(yawPrev[-1]), color=\"red\", scale=8.0, width=0.015) # vehicle pose\n",
    "plt.scatter(xNext * gridSize, yNext * gridSize, color='purple')\n",
    "plt.quiver(goalStateNonscaled[numTrain + vizIdx, 0] * gridSize, goalStateNonscaled[numTrain + vizIdx, 1] * gridSize, np.cos(goalStateNonscaled[numTrain + vizIdx, 2]), np.sin(goalStateNonscaled[numTrain + vizIdx, 2]), color=\"blue\", scale=8.0, width=0.015) # goal\n",
    "plt.imshow(mapImg, extent=[0, gridSize, 0, gridSize], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(t, y_viz, s=70, alpha=0.01)\n",
    "plt.scatter(t[0], steerNext, alpha=0.4)\n",
    "plt.scatter(np.arange(-nPreviousStates, 0) * 0.05, steerPrev)\n",
    "plt.ylim([-1.,1.])\n",
    "plt.legend(['Predict', 'True', 'Previous'])\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('Steering')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network, \"/home/oscar_palfelt/MSc_thesis/EECS_Degree_Project/learn_control/networks/ctrl.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e6cc1179d3ed6ae143ff3dc9ac563dc8e696a51d4136aae17def4ab6c30aa1f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('project_env_3_8': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

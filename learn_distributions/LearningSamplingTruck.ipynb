{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Workspace problem with several narrow gaps\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "import csv\n",
    "from random import randint, random\n",
    "import time\n",
    "\n",
    "\n",
    "cuda = True\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural network parameters\n",
    "mb_size = 256 # mini batch dim\n",
    "h_Q_dim = 512 # encoder dim\n",
    "h_P_dim = 512 # decoder dim\n",
    "\n",
    "c = 0 # unsure of this constant\n",
    "lr = 1e-5 # learning rate\n",
    "\n",
    "# problem dimenc_dimsions\n",
    "dim = 5 # (x, y, yaw, xdot, ydot)\n",
    "dataElements = dim+3*3+2*dim # sample (5D), gap1 (2D, 1D orientation), gap2, gap3, init (5D), goal (5D)\n",
    "\n",
    "z_dim = 2 # latent dim\n",
    "X_dim = dim # samples dim\n",
    "y_dim = dim # reconstruction of the original point (unsused?)\n",
    "c_dim = dataElements - dim # dimension of conditioning variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in data from csv\n",
    "filename = '/home/oscar_palfelt/MSc_thesis/EECS_Degree_Project/motion_planning/pathData'\n",
    "\n",
    "data = np.genfromtxt(filename, delimiter=',', dtype='d', usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23])\n",
    "numEntries = data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the inputs and conditions into test train (to be processed in the next step into an occupancy grid representation)\n",
    "ratioTestTrain = 0.8;\n",
    "numTrain = int(numEntries*ratioTestTrain)\n",
    "\n",
    "X_train = data[0:numTrain,0:dim] # samples state: x, y, xdot, ydot\n",
    "c_train = data[0:numTrain,dim:dataElements] # conditions: gaps, init (4), goal (4)\n",
    "\n",
    "X_test = data[numTrain:numEntries,0:dim]\n",
    "c_test = data[numTrain:numEntries,dim:dataElements]\n",
    "numTest = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Iter: 5000\n",
      "Iter: 10000\n",
      "Time:  4.795232534408569\n"
     ]
    }
   ],
   "source": [
    "# change conditions to occupancy grid\n",
    "def isSampleFree(sample, obs):\n",
    "    for o in list(range(0,obs.shape[0]//(2*dimW))): # python 2 -> 3: use list(), use //\n",
    "        isFree = 0\n",
    "        for d in range(0,sample.shape[0]):\n",
    "            if (sample[d] < obs[2*dimW*o + d] or sample[d] > obs[2*dimW*o + d + dimW]):\n",
    "                isFree = 1\n",
    "                break\n",
    "        if isFree == 0:\n",
    "            return 0\n",
    "    return 1\n",
    "\n",
    "gridSize = 11\n",
    "dimW = 3\n",
    "plotOn = False;\n",
    "\n",
    "# process data into occupancy grid\n",
    "conditions = data[0:numEntries,dim:dataElements]\n",
    "conditionsOcc = np.zeros([numEntries,gridSize*gridSize])\n",
    "occGridSamples = np.zeros([gridSize*gridSize, 2])\n",
    "gridPointsRange = np.linspace(0,1,num=gridSize)\n",
    "\n",
    "idx = 0;\n",
    "for i in gridPointsRange:\n",
    "    for j in gridPointsRange:\n",
    "        occGridSamples[idx,0] = i\n",
    "        occGridSamples[idx,1] = j\n",
    "        idx += 1;\n",
    "\n",
    "start = time.time();\n",
    "for j in range(0,numEntries,1):\n",
    "    dw = 0.1\n",
    "    dimW = 3\n",
    "    gap1 = conditions[j,0:3]\n",
    "    gap2 = conditions[j,3:6]\n",
    "    gap3 = conditions[j,6:9]\n",
    "\n",
    "    obs1 = [0, gap1[1]-dw, -0.5,             gap1[0], gap1[1], 1.5]\n",
    "    obs2 = [gap2[0]-dw, 0, -0.5,             gap2[0], gap2[1], 1.5];\n",
    "    obs3 = [gap2[0]-dw, gap2[1]+dw, -0.5,    gap2[0], 1, 1.5];\n",
    "    obs4 = [gap1[0]+dw, gap1[1]-dw, -0.5,    gap3[0], gap1[1], 1.5];\n",
    "    obs5 = [gap3[0]+dw, gap1[1]-dw, -0.5,    1, gap1[1], 1.5];\n",
    "    obs = np.concatenate((obs1, obs2, obs3, obs4, obs5), axis=0)\n",
    "    \n",
    "    if j % 5000 == 0:\n",
    "        print('Iter: {}'.format(j))\n",
    "        \n",
    "    occGrid = np.zeros(gridSize*gridSize)\n",
    "    for i in range(0,gridSize*gridSize):\n",
    "        occGrid[i] = isSampleFree(occGridSamples[i,:],obs)\n",
    "    conditionsOcc[j,:] = occGrid\n",
    "    \n",
    "end = time.time();\n",
    "print('Time: ', end-start)\n",
    "    \n",
    "cs = np.concatenate((conditionsOcc, data[0:numEntries,dim+3*dimW:dataElements]), axis=1) # occ(11x11), init (6D), goal (6D)\n",
    "c_dim = cs.shape[1]\n",
    "c_gapsInitGoal = c_test\n",
    "c_train = cs[0:numTrain,:] \n",
    "c_test = cs[numTrain:numEntries,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pytorch networks\n",
    "# based on https://github.com/Jackson-Kang/Pytorch-VAE-tutorial/blob/master/.ipynb_checkpoints/01_Variational_AutoEncoder-checkpoint.ipynb\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=X_dim+c_dim, hidden_dim=h_Q_dim, latent_dim=z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.z_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        seq = self.network(x)\n",
    "\n",
    "        return self.z_mu(seq), self.z_logvar(seq)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=z_dim+c_dim, hidden_dim=h_P_dim, output_dim=X_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "    \n",
    "    def reparameterization(self, mean, logvar):\n",
    "        epsilon = torch.randn_like(mean).to(DEVICE)        # sampling epsilon        \n",
    "        z = mean + torch.exp(0.5 * logvar) * epsilon       # reparameterization trick\n",
    "        return z\n",
    "\n",
    "    def forward(self, x, c, encode=True):\n",
    "        if encode:\n",
    "            z_mu, z_logvar = self.Encoder(torch.cat((x, c), dim=1))\n",
    "            z = self.reparameterization(z_mu, z_logvar)\n",
    "\n",
    "            y = self.Decoder(torch.cat((z, c), dim=1))\n",
    "            \n",
    "            return y, z_mu, z_logvar\n",
    "        else:\n",
    "            z = x\n",
    "            y = self.Decoder(torch.cat((z, c), dim=1))    \n",
    "\n",
    "            return y\n",
    "\n",
    "\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "network = NeuralNetwork(Encoder=encoder, Decoder=decoder).to(DEVICE)\n",
    "\n",
    "weight = torch.tensor([1, 1, 0.5, 0.5, 0.5], requires_grad=False, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def loss_function(x, y, mean, logvar):\n",
    "    recon_loss = (weight * (x - y) ** 2).mean()\n",
    "    kl_loss    = 10**-4 * 2 * torch.sum(torch.exp(logvar) + mean.pow(2) - 1. - logvar, dim=1)\n",
    "\n",
    "    return torch.mean(kl_loss + recon_loss)\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Loss: 0.4882\n",
      "\n",
      "Iter: 1000\n",
      "Loss: 0.04835\n",
      "\n",
      "Iter: 2000\n",
      "Loss: 0.03678\n",
      "\n",
      "Iter: 3000\n",
      "Loss: 0.03428\n",
      "\n",
      "Iter: 4000\n",
      "Loss: 0.03065\n",
      "\n",
      "Iter: 5000\n",
      "Loss: 0.02598\n",
      "\n",
      "Iter: 6000\n",
      "Loss: 0.02679\n",
      "\n",
      "Iter: 7000\n",
      "Loss: 0.02421\n",
      "\n",
      "Iter: 8000\n",
      "Loss: 0.02302\n",
      "\n",
      "Iter: 9000\n",
      "Loss: 0.02201\n",
      "\n",
      "Iter: 10000\n",
      "Loss: 0.0209\n",
      "\n",
      "Iter: 11000\n",
      "Loss: 0.02008\n",
      "\n",
      "Iter: 12000\n",
      "Loss: 0.01893\n",
      "\n",
      "Iter: 13000\n",
      "Loss: 0.01729\n",
      "\n",
      "Iter: 14000\n",
      "Loss: 0.01864\n",
      "\n",
      "Iter: 15000\n",
      "Loss: 0.01758\n",
      "\n",
      "Iter: 16000\n",
      "Loss: 0.01765\n",
      "\n",
      "Iter: 17000\n",
      "Loss: 0.01811\n",
      "\n",
      "Iter: 18000\n",
      "Loss: 0.01542\n",
      "\n",
      "Iter: 19000\n",
      "Loss: 0.01619\n",
      "\n",
      "Iter: 20000\n",
      "Loss: 0.01601\n",
      "\n",
      "Iter: 21000\n",
      "Loss: 0.01547\n",
      "\n",
      "Iter: 22000\n",
      "Loss: 0.01461\n",
      "\n",
      "Iter: 23000\n",
      "Loss: 0.01445\n",
      "\n",
      "Iter: 24000\n",
      "Loss: 0.01484\n",
      "\n",
      "Iter: 25000\n",
      "Loss: 0.01441\n",
      "\n",
      "Iter: 26000\n",
      "Loss: 0.0144\n",
      "\n",
      "Iter: 27000\n",
      "Loss: 0.01306\n",
      "\n",
      "Iter: 28000\n",
      "Loss: 0.01412\n",
      "\n",
      "Iter: 29000\n",
      "Loss: 0.01381\n",
      "\n",
      "Iter: 30000\n",
      "Loss: 0.01357\n",
      "\n",
      "Iter: 31000\n",
      "Loss: 0.0133\n",
      "\n",
      "Iter: 32000\n",
      "Loss: 0.01422\n",
      "\n",
      "Iter: 33000\n",
      "Loss: 0.01289\n",
      "\n",
      "Iter: 34000\n",
      "Loss: 0.01193\n",
      "\n",
      "Iter: 35000\n",
      "Loss: 0.01275\n",
      "\n",
      "Iter: 36000\n",
      "Loss: 0.01252\n",
      "\n",
      "Iter: 37000\n",
      "Loss: 0.01204\n",
      "\n",
      "Iter: 38000\n",
      "Loss: 0.01271\n",
      "\n",
      "Iter: 39000\n",
      "Loss: 0.01186\n",
      "\n",
      "Iter: 40000\n",
      "Loss: 0.01192\n",
      "\n",
      "Iter: 41000\n",
      "Loss: 0.01175\n",
      "\n",
      "Iter: 42000\n",
      "Loss: 0.01147\n",
      "\n",
      "Iter: 43000\n",
      "Loss: 0.0119\n",
      "\n",
      "Iter: 44000\n",
      "Loss: 0.01125\n",
      "\n",
      "Iter: 45000\n",
      "Loss: 0.01113\n",
      "\n",
      "Iter: 46000\n",
      "Loss: 0.01114\n",
      "\n",
      "Iter: 47000\n",
      "Loss: 0.01202\n",
      "\n",
      "Iter: 48000\n",
      "Loss: 0.01134\n",
      "\n",
      "Iter: 49000\n",
      "Loss: 0.01193\n",
      "\n",
      "Iter: 50000\n",
      "Loss: 0.01077\n",
      "\n",
      "Iter: 51000\n",
      "Loss: 0.0115\n",
      "\n",
      "Iter: 52000\n",
      "Loss: 0.01092\n",
      "\n",
      "Iter: 53000\n",
      "Loss: 0.01091\n",
      "\n",
      "Iter: 54000\n",
      "Loss: 0.01101\n",
      "\n",
      "Iter: 55000\n",
      "Loss: 0.01085\n",
      "\n",
      "Iter: 56000\n",
      "Loss: 0.01075\n",
      "\n",
      "Iter: 57000\n",
      "Loss: 0.01098\n",
      "\n",
      "Iter: 58000\n",
      "Loss: 0.0106\n",
      "\n",
      "Iter: 59000\n",
      "Loss: 0.01022\n",
      "\n",
      "Iter: 60000\n",
      "Loss: 0.0105\n",
      "\n",
      "Iter: 61000\n",
      "Loss: 0.009785\n",
      "\n",
      "Iter: 62000\n",
      "Loss: 0.01075\n",
      "\n",
      "Iter: 63000\n",
      "Loss: 0.009938\n",
      "\n",
      "Iter: 64000\n",
      "Loss: 0.01123\n",
      "\n",
      "Iter: 65000\n",
      "Loss: 0.01023\n",
      "\n",
      "Iter: 66000\n",
      "Loss: 0.0102\n",
      "\n",
      "Iter: 67000\n",
      "Loss: 0.01008\n",
      "\n",
      "Iter: 68000\n",
      "Loss: 0.009104\n",
      "\n",
      "Iter: 69000\n",
      "Loss: 0.01077\n",
      "\n",
      "Iter: 70000\n",
      "Loss: 0.008755\n",
      "\n",
      "Iter: 71000\n",
      "Loss: 0.009728\n",
      "\n",
      "Iter: 72000\n",
      "Loss: 0.009211\n",
      "\n",
      "Iter: 73000\n",
      "Loss: 0.009319\n",
      "\n",
      "Iter: 74000\n",
      "Loss: 0.009371\n",
      "\n",
      "Iter: 75000\n",
      "Loss: 0.009655\n",
      "\n",
      "Iter: 76000\n",
      "Loss: 0.009383\n",
      "\n",
      "Iter: 77000\n",
      "Loss: 0.008966\n",
      "\n",
      "Iter: 78000\n",
      "Loss: 0.009356\n",
      "\n",
      "Iter: 79000\n",
      "Loss: 0.009543\n",
      "\n",
      "Iter: 80000\n",
      "Loss: 0.009988\n",
      "\n",
      "Iter: 81000\n",
      "Loss: 0.008766\n",
      "\n",
      "Iter: 82000\n",
      "Loss: 0.008429\n",
      "\n",
      "Iter: 83000\n",
      "Loss: 0.008812\n",
      "\n",
      "Iter: 84000\n",
      "Loss: 0.009738\n",
      "\n",
      "Iter: 85000\n",
      "Loss: 0.008964\n",
      "\n",
      "Iter: 86000\n",
      "Loss: 0.009273\n",
      "\n",
      "Iter: 87000\n",
      "Loss: 0.009042\n",
      "\n",
      "Iter: 88000\n",
      "Loss: 0.009439\n",
      "\n",
      "Iter: 89000\n",
      "Loss: 0.008723\n",
      "\n",
      "Iter: 90000\n",
      "Loss: 0.008681\n",
      "\n",
      "Iter: 91000\n",
      "Loss: 0.008685\n",
      "\n",
      "Iter: 92000\n",
      "Loss: 0.008572\n",
      "\n",
      "Iter: 93000\n",
      "Loss: 0.009181\n",
      "\n",
      "Iter: 94000\n",
      "Loss: 0.009294\n",
      "\n",
      "Iter: 95000\n",
      "Loss: 0.008628\n",
      "\n",
      "Iter: 96000\n",
      "Loss: 0.008835\n",
      "\n",
      "Iter: 97000\n",
      "Loss: 0.009107\n",
      "\n",
      "Iter: 98000\n",
      "Loss: 0.008357\n",
      "\n",
      "Iter: 99000\n",
      "Loss: 0.007974\n",
      "\n",
      "Iter: 100000\n",
      "Loss: 0.008482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for it in range(100001):\n",
    "    # randomly generate batches\n",
    "    batch_elements = [randint(0,numTrain-1) for n in range(0,mb_size)]\n",
    "\n",
    "    X_mb = torch.tensor(X_train[batch_elements,:], requires_grad=True, dtype=torch.float32, device=DEVICE)\n",
    "    c_mb = torch.tensor(c_train[batch_elements,:], requires_grad=True, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y, z_mu, z_logvar = network(X_mb, c_mb)\n",
    "    \n",
    "    loss = loss_function(X_mb, y, z_mu, z_logvar)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Loss: {:.4}'. format(loss))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'randint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/oscar_palfelt/MSc_thesis/EECS_Degree_Project/learn_distributions/LearningSamplingTruck.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oscar_palfelt/MSc_thesis/EECS_Degree_Project/learn_distributions/LearningSamplingTruck.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39m# plot the latent space\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oscar_palfelt/MSc_thesis/EECS_Degree_Project/learn_distributions/LearningSamplingTruck.ipynb#ch0000007?line=2'>3</a>\u001b[0m num_viz \u001b[39m=\u001b[39m \u001b[39m3000\u001b[39m \u001b[39m# number of samples to draw in latent space\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/oscar_palfelt/MSc_thesis/EECS_Degree_Project/learn_distributions/LearningSamplingTruck.ipynb#ch0000007?line=3'>4</a>\u001b[0m vizIdx \u001b[39m=\u001b[39m randint(\u001b[39m0\u001b[39m,numTest\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m); \u001b[39m# chose a random test scenario\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oscar_palfelt/MSc_thesis/EECS_Degree_Project/learn_distributions/LearningSamplingTruck.ipynb#ch0000007?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(vizIdx)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oscar_palfelt/MSc_thesis/EECS_Degree_Project/learn_distributions/LearningSamplingTruck.ipynb#ch0000007?line=7'>8</a>\u001b[0m c_sample_seed \u001b[39m=\u001b[39m c_test[vizIdx,:]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'randint' is not defined"
     ]
    }
   ],
   "source": [
    "# plot the latent space\n",
    "\n",
    "num_viz = 3000 # number of samples to draw in latent space\n",
    "vizIdx = randint(0,numTest-1); # chose a random test scenario\n",
    "\n",
    "print(vizIdx)\n",
    "\n",
    "c_sample_seed = c_test[vizIdx,:]\n",
    "c_sample = torch.from_numpy(np.repeat([c_sample_seed],num_viz,axis=0)).float().to(DEVICE)\n",
    "c_viz = c_gapsInitGoal[vizIdx,:]\n",
    "\n",
    "# directly sample from the latent space (preferred, what we will use in the end)\n",
    "y_viz = network(torch.randn(num_viz, z_dim).to(DEVICE), c_sample, encode=False).cpu().detach().numpy() # y_viz is sample in state space, zviz in latent space (3D)\n",
    "\n",
    "fig1 = plt.figure(figsize=(10,6), dpi=80)\n",
    "ax1 = fig1.add_subplot(111, aspect='equal')\n",
    "\n",
    "#plt.quiver(y_viz[:,0], y_viz[:,1], np.cos(y_viz[:,2]), np.sin(y_viz[:,2]), color=\"green\", alpha=0.1) # steer right -> Blue, steer left -> Red\n",
    "plt.scatter(y_viz[:,0],y_viz[:,1], color=\"green\", s=70, alpha=0.1)\n",
    "\n",
    "dw = 0.1\n",
    "dimW = 3\n",
    "gap1 = c_viz[0:3]\n",
    "gap2 = c_viz[3:6]\n",
    "gap3 = c_viz[6:9]\n",
    "init = c_viz[9:14]\n",
    "goal = c_viz[14:19]\n",
    "\n",
    "obs1 = [0, gap1[1]-dw, -0.5,             gap1[0], gap1[1], 1.5]\n",
    "obs2 = [gap2[0]-dw, 0, -0.5,             gap2[0], gap2[1], 1.5];\n",
    "obs3 = [gap2[0]-dw, gap2[1]+dw, -0.5,    gap2[0], 1, 1.5];\n",
    "obs4 = [gap1[0]+dw, gap1[1]-dw, -0.5,    gap3[0], gap1[1], 1.5];\n",
    "obs5 = [gap3[0]+dw, gap1[1]-dw, -0.5,    1, gap1[1], 1.5];\n",
    "obsBounds = [-0.1, -0.1, -0.5, 0, 1.1, 1.5,\n",
    "            -0.1, -0.1, -0.5, 1.1, 0, 1.5,\n",
    "            -0.1, 1, -0.5, 1.1, 1.1, 1.5,\n",
    "            1, -0.1, -0.5, 1.1, 1.1, 1.5,]\n",
    "\n",
    "obs = np.concatenate((obs1, obs2, obs3, obs4, obs5, obsBounds), axis=0)\n",
    "for i in list(range(0,obs.shape[0]//(2*dimW))): # list() and //\n",
    "    ax1.add_patch(\n",
    "    patches.Rectangle(\n",
    "        (obs[i*2*dimW], obs[i*2*dimW+1]),   # (x,y)\n",
    "        obs[i*2*dimW+dimW] - obs[i*2*dimW],          # width\n",
    "        obs[i*2*dimW+dimW+1] - obs[i*2*dimW+1],          # height\n",
    "        alpha=0.6\n",
    "    ))\n",
    "    \n",
    "for i in list(range(0,gridSize*gridSize)): # plot occupancy grid | list() and //\n",
    "    cIdx = i\n",
    "    if c_sample_seed[cIdx] == 0:\n",
    "        plt.scatter(occGridSamples[i,0], occGridSamples[i,1], color=\"red\", s=50, alpha=0.7)\n",
    "    else:\n",
    "        plt.scatter(occGridSamples[i,0], occGridSamples[i,1], color=\"green\", s=50, alpha=0.7)\n",
    "\n",
    "plt.scatter(init[0], init[1], color=\"red\", s=250, edgecolors='black') # init\n",
    "plt.scatter(goal[0], goal[1], color=\"blue\", s=250, edgecolors='black') # goal\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64b15a9dfc037f101e6686cd2beac8a44adf976402fd7d9f80c174bf7ce5b31d"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
